[label_model]

; The number of epochs to train (where each epoch is a single optimization step)
n_epochs = 1000

; Which optimizer to use (one of ["sgd", "adam", "adamax"])
; Choosing the optimizer will automatically update the recommended lr, l2, and lr_scheduler. If you really want to
; tinker with these, edit their values in utils/config.py
optimizer = sgd

; LF precision initializations / priors
prec_init = 0.7

; Report loss every this many epochs (steps)
log_freq = 100
