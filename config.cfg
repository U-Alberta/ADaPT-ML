[label_model]

; The number of epochs to train (where each epoch is a single optimization step)
n_epochs = 1000

; Which optimizer to use (one of ["sgd", "adam", "adamax"])
; Choosing the optimizer will automatically update the recommended lr, l2, and lr_scheduler. If you really want to
; tinker with these, edit their values in utils/config.py
optimizer = sgd

; LF precision initializations / priors
prec_init = 0.7

; Report loss every this many epochs (steps)
log_freq = 100


[mlogit]

; Algorithm to use in the optimization problem.
solver = lbfgs

; Tolerance for stopping criteria.
tol = 1e-4

; Inverse of regularization strength; must be a positive float. Like in support vector machines,
; smaller values specify stronger regularization.
C = 1.0

; Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.
fit_intercept = True

; Maximum number of iterations taken for the solvers to converge.
max_iter = 100

; If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’
; the loss minimised is the multinomial loss fit across the entire probability distribution,
; even when the data is binary.
multi_class = auto

; For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.
verbose = 10000

; When set to True, reuse the solution of the previous call to fit as initialization, otherwise,
; just erase the previous solution.
warm_start = False

; Number of CPU cores used when parallelizing over classes if multi_class=’ovr’”.
; This parameter is ignored when the solver is set to ‘liblinear’ regardless of whether ‘multi_class’
; is specified or not. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.
n_jobs = -1
